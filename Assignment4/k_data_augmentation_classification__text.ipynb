{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAuVtp8fppKhx7uXwrmIMe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananya-AJ/Deep-Learning/blob/main/Assignment4/k_data_augmentation_classification__text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8rgRipZZiG8",
        "outputId": "989ca07f-a93b-4b0a-8464-d233f339d435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "imdb = keras.datasets.imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocess the data by padding the sequences to a uniform length and creating a validation set"
      ],
      "metadata": {
        "id": "VROXVt9FZ8Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "TK1TFOCmaEU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "RJaHG3i8acfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 200\n",
        "train_data = pad_sequences(train_data, maxlen=maxlen)\n",
        "test_data = pad_sequences(test_data, maxlen=maxlen)\n",
        "\n",
        "x_val = train_data[:10000]\n",
        "partial_x_train = train_data[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "partial_y_train = train_labels[10000:]\n"
      ],
      "metadata": {
        "id": "1iXb_Hc1Z37o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "define the model using an embedding layer, a global max pooling layer, and two dense layers with dropout regularization"
      ],
      "metadata": {
        "id": "Gx3GZDS4aM28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "embedding_dim = 16\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(10000, embedding_dim, input_length=maxlen),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "brtfd2DwaAq_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model with data augmentation using Keras.\n",
        "During training, Keras will automatically apply data augmentation techniques like shuffling and random cropping to the input data to prevent overfitting\n",
        "\n"
      ],
      "metadata": {
        "id": "RicLzmMFalZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    partial_x_train,\n",
        "    partial_y_train,\n",
        "    epochs=40,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_val, y_val)\n",
        ")\n"
      ],
      "metadata": {
        "id": "Z_k--DU7ageK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it_pTaCvaq1-",
        "outputId": "c474e91b-7ff3-48cc-dd00-c9f594166bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5842 - accuracy: 0.8201\n",
            "[0.5842366218566895, 0.8200799822807312]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation using nlpaug, replacing words with synonyms"
      ],
      "metadata": {
        "id": "rXxAjcg02-jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug"
      ],
      "metadata": {
        "id": "JOGNucVmsqvN",
        "outputId": "846fe4ce-82ce-49ff-c967-e066ee407a26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (2.27.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.4.4)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (4.6.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (3.10.7)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (2022.12.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "# Define the augmentation function\n",
        "def augment_text(text, aug):\n",
        "    augmented_text = aug.augment(text)\n",
        "    return augmented_text\n",
        "\n",
        "# Define the augmentation method\n",
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "\n",
        "# Augment the text data\n",
        "augmented_x_train = [aug.augment(str(text)) for text in partial_x_train]\n",
        "augmented_x_val = [aug.augment(str(text)) for text in x_val]\n",
        "augmented_test_data = [aug.augment(str(text)) for text in test_data]\n"
      ],
      "metadata": {
        "id": "cWyzmhoDtad6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the augmented text data into sequences of integers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Create a tokenizer object\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "augmented_x_train = tokenizer.texts_to_sequences(augmented_x_train)\n",
        "augmented_x_val = tokenizer.texts_to_sequences(augmented_x_val)\n",
        "augmented_test_data = tokenizer.texts_to_sequences(augmented_test_data)\n",
        "\n",
        "# Pad the augmented data\n",
        "augmented_x_train = pad_sequences(augmented_x_train, maxlen=maxlen)\n",
        "augmented_x_val = pad_sequences(augmented_x_val, maxlen=maxlen)\n",
        "augmented_test_data = pad_sequences(augmented_test_data, maxlen=maxlen)\n"
      ],
      "metadata": {
        "id": "L-O5g-Ggze4o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with augmented data\n",
        "history = model.fit(\n",
        "    augmented_x_train,\n",
        "    partial_y_train,\n",
        "    epochs=40,\n",
        "    batch_size=512,\n",
        "    validation_data=(augmented_x_val, y_val)\n",
        ")\n",
        "\n",
        "# Evaluate the model with the augmented test data\n",
        "results = model.evaluate(augmented_test_data, test_labels)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "BWN3dREGz1yR",
        "outputId": "c16c5c88-8cb3-4a4c-d8cb-ef402458ff9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 2s 32ms/step - loss: 0.6932 - accuracy: 0.4985 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.6932 - accuracy: 0.4973 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 1s 30ms/step - loss: 0.6931 - accuracy: 0.5046 - val_loss: 0.6934 - val_accuracy: 0.4947\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.6932 - accuracy: 0.5017 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 0.6933 - accuracy: 0.4993 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.6932 - accuracy: 0.4942 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 1s 47ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 1s 30ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.5000\n",
            "[0.6931670904159546, 0.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With data augmentation(replacing words with synonyms), accuracy reduced from 82% to 50%"
      ],
      "metadata": {
        "id": "hcVx4Wfv3LgP"
      }
    }
  ]
}