{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPtLSJ6gCgXM8gqBrDLqVT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananya-AJ/Deep-Learning/blob/main/Assignment4/k_data_augmentation_classification__Document.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. The dataset is widely used for text classification.Some of the newsgroup categories include politics, religion, science, and sports."
      ],
      "metadata": {
        "id": "VddvJTxKeswN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nlpaug.augmenter.word as naw\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "4RnimJetmxIB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz\n",
        "!tar xvzf 20news-18828.tar.gz"
      ],
      "metadata": {
        "id": "cTy4w4FPfDp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load the \"20 Newsgroups\" dataset and train a simple neural network on it using PyTorch"
      ],
      "metadata": {
        "id": "MyAVV98hgFfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# download and load the 20 Newsgroups dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# convert text data to numerical features using a bag-of-words model\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data).toarray()\n",
        "X_test = vectorizer.transform(newsgroups_test.data).toarray()\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "# convert data to PyTorch tensors\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# define a simple neural network model\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(1000, 100)\n",
        "        self.fc2 = torch.nn.Linear(100, 20)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# create an instance of the neural network model\n",
        "net = Net()\n",
        "\n",
        "# define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# train the neural network\n",
        "for epoch in range(100):\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}, loss: {loss.item():.4f}')\n",
        "\n",
        "# evaluate the neural network on the test data\n",
        "with torch.no_grad():\n",
        "    outputs = net(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
        "    print(f'Test accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRjyWDYeewrx",
        "outputId": "d94a943c-f803-4415-de2a-002389175464"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, loss: 2.9993\n",
            "Epoch 20, loss: 2.9964\n",
            "Epoch 30, loss: 2.9935\n",
            "Epoch 40, loss: 2.9907\n",
            "Epoch 50, loss: 2.9880\n",
            "Epoch 60, loss: 2.9854\n",
            "Epoch 70, loss: 2.9828\n",
            "Epoch 80, loss: 2.9803\n",
            "Epoch 90, loss: 2.9779\n",
            "Epoch 100, loss: 2.9755\n",
            "Test accuracy: 0.0659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation using nlpaug. naw.SynonymAug is a data augmentation technique provided by the nlpaug library. It performs word replacement by synonyms, which are words with the same or similar meanings."
      ],
      "metadata": {
        "id": "R7qbb7d1gvfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rL0AgfQe19x",
        "outputId": "022de522-8a9d-4391-c767-912250ec5eed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (4.6.6)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (2.27.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.4.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.65.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (3.10.7)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (2022.12.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W43mgfwlMY7",
        "outputId": "9aae24fe-2462-4767-d12c-ab0aec40db8c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.utils._bunch.Bunch'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nlpaug.augmenter.word as naw\n",
        "# Define the text augmentation function\n",
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "augmented_data = []\n",
        "for text in newsgroups_train[\"data\"]:\n",
        "    augmented_text = aug.augment(text)\n",
        "    augmented_text = ' '.join(augmented_text)  # Convert list of words to string\n",
        "    augmented_data.append(augmented_text)"
      ],
      "metadata": {
        "id": "QiYawSGDhQQw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create bag-of-words representations of the augmented text data\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=10000)\n",
        "train_data_bow = vectorizer.fit_transform(augmented_data)\n",
        "test_data_bow = vectorizer.transform(newsgroups_test.data)"
      ],
      "metadata": {
        "id": "kQLmUJk9otF8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_bow = train_data_bow.sorted_indices()"
      ],
      "metadata": {
        "id": "aEE5jdkCqVHN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_shape=(train_data_bow.shape[1],), activation='relu'))\n",
        "model.add(Dense(20, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "model.fit(train_data_bow, newsgroups_train.target, batch_size=batch_size, epochs=epochs, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEuDMmucoCG2",
        "outputId": "1eb82bb8-6323-406f-c21a-eb11bc7971c2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "354/354 [==============================] - 5s 12ms/step - loss: 1.9308 - accuracy: 0.5846\n",
            "Epoch 2/10\n",
            "354/354 [==============================] - 5s 15ms/step - loss: 0.8984 - accuracy: 0.8253\n",
            "Epoch 3/10\n",
            "354/354 [==============================] - 6s 16ms/step - loss: 0.5648 - accuracy: 0.8997\n",
            "Epoch 4/10\n",
            "354/354 [==============================] - 5s 14ms/step - loss: 0.3794 - accuracy: 0.9318\n",
            "Epoch 5/10\n",
            "354/354 [==============================] - 5s 14ms/step - loss: 0.2773 - accuracy: 0.9495\n",
            "Epoch 6/10\n",
            "354/354 [==============================] - 5s 13ms/step - loss: 0.2184 - accuracy: 0.9598\n",
            "Epoch 7/10\n",
            "354/354 [==============================] - 7s 18ms/step - loss: 0.1799 - accuracy: 0.9631\n",
            "Epoch 8/10\n",
            "354/354 [==============================] - 4s 11ms/step - loss: 0.1584 - accuracy: 0.9661\n",
            "Epoch 9/10\n",
            "354/354 [==============================] - 5s 15ms/step - loss: 0.1402 - accuracy: 0.9677\n",
            "Epoch 10/10\n",
            "354/354 [==============================] - 6s 16ms/step - loss: 0.1292 - accuracy: 0.9687\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f16c4f5c9a0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_data_bow, newsgroups_test.target)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NdLYyg5q4WI",
        "outputId": "281b2398-694f-4fb9-b248-9c9f82d84645"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "236/236 [==============================] - 1s 3ms/step - loss: 1.9001 - accuracy: 0.6414\n",
            "Test accuracy: 0.641396701335907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see test accuracy remained same but the loss reduced considerably"
      ],
      "metadata": {
        "id": "eStQbobfqef9"
      }
    }
  ]
}